{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaNOKapWJD9N"
      },
      "source": [
        "# **Comparative Analysis of Bi-LSTM + CRF and BERT for Named Entity Recognition**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4yc4pRBI4nK"
      },
      "source": [
        "## Method two:BERT for Named Entity Recognition\n",
        "\n",
        "Steps\n",
        "\n",
        "1. Prepare training data and map labels  \n",
        "2. Load pretrained BERT model(bert for ner) and tokenizer\n",
        "3. Define training arguments and trainer\n",
        "4. Fine-tune model on training data\n",
        "5. Evaluate on test data\n",
        "\n",
        "The trained model can extract named entities from text by encoding the text and applying the model's token classification head.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezwryqgSXuSO"
      },
      "source": [
        "# Importing packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:48.726563Z",
          "iopub.status.busy": "2023-09-29T14:48:48.726316Z",
          "iopub.status.idle": "2023-09-29T14:48:49.041760Z",
          "shell.execute_reply": "2023-09-29T14:48:49.040870Z",
          "shell.execute_reply.started": "2023-09-29T14:48:48.726539Z"
        },
        "id": "QLvm1cRmI4nN",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "import evaluate\n",
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "from transformers import pipeline\n",
        "import time\n",
        "from memory_profiler import memory_usage\n",
        "from transformers import AutoModelForTokenClassification, AutoConfig\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure nltk resources are downloaded\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUX7GUeSXy9l"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:49.043979Z",
          "iopub.status.busy": "2023-09-29T14:48:49.043351Z",
          "iopub.status.idle": "2023-09-29T14:48:56.560663Z",
          "shell.execute_reply": "2023-09-29T14:48:56.559760Z",
          "shell.execute_reply.started": "2023-09-29T14:48:49.043946Z"
        },
        "id": "fP1Zf9VeI4nP",
        "outputId": "75bf7f5e-9cca-4e86-bae6-5b4ac7988108",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the CoNLL-2003 dataset\n",
        "dataset = load_dataset('conll2003')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfNpXFEmX8KZ"
      },
      "source": [
        "# Understaning the structre of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:56.562791Z",
          "iopub.status.busy": "2023-09-29T14:48:56.562089Z",
          "iopub.status.idle": "2023-09-29T14:48:56.572558Z",
          "shell.execute_reply": "2023-09-29T14:48:56.571303Z",
          "shell.execute_reply.started": "2023-09-29T14:48:56.562758Z"
        },
        "id": "ma8ygHElI4nP",
        "outputId": "2b8c5354-75f2-4478-e4e0-269afe5756c1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None), 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}\n"
          ]
        }
      ],
      "source": [
        "# Printing the features of the training dataset.\n",
        "print(dataset['train'].features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:56.576711Z",
          "iopub.status.busy": "2023-09-29T14:48:56.576303Z",
          "iopub.status.idle": "2023-09-29T14:48:56.611135Z",
          "shell.execute_reply": "2023-09-29T14:48:56.610170Z",
          "shell.execute_reply.started": "2023-09-29T14:48:56.576664Z"
        },
        "id": "ynD_ZYnyI4nQ",
        "outputId": "d38e7374-43cf-43e0-f028-92816f387eb2",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Accessing the label names from the 'ner_tags' feature.\n",
        "label_names = dataset['train'].features['ner_tags'].feature.names\n",
        "label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:56.613961Z",
          "iopub.status.busy": "2023-09-29T14:48:56.613273Z",
          "iopub.status.idle": "2023-09-29T14:48:56.623275Z",
          "shell.execute_reply": "2023-09-29T14:48:56.622127Z",
          "shell.execute_reply.started": "2023-09-29T14:48:56.613931Z"
        },
        "id": "t4hPok2lI4nQ",
        "outputId": "1c799bba-3d55-44c7-b62f-0d31bbb32890",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fQz2-pbYC3g"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "A pretrained BERT tokenizer is used to tokenize the text, ensuring that the input is compatible with the BERT model. Special care is taken to align the tokenized words with their respective labels, as BERT tokenizes text into subword units, which may cause a mismatch between the original words and their labels.\n",
        "\n",
        "To address this issue:\n",
        "\n",
        "- Labels are adjusted so that each token, including special tokens like [CLS] and [SEP], receives the appropriate label.\n",
        "- The tokenized data is grouped into batches, ensuring efficient processing.\n",
        "\n",
        "This preprocessing step prepares the data for training and evaluation, maintaining consistency between the tokenized inputs and their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:56.625261Z",
          "iopub.status.busy": "2023-09-29T14:48:56.624519Z",
          "iopub.status.idle": "2023-09-29T14:48:59.953903Z",
          "shell.execute_reply": "2023-09-29T14:48:59.952990Z",
          "shell.execute_reply.started": "2023-09-29T14:48:56.625218Z"
        },
        "id": "MgDeEnsPI4nR",
        "outputId": "7aff6605-9c8a-4670-ae2f-442c1625e041",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\visha\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define the checkpoint to use for the tokenizer.\n",
        "model_name = 'bert-base-cased'\n",
        "\n",
        "# Creating a tokenizer instance by loading the pre-trained checkpoint.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:59.955384Z",
          "iopub.status.busy": "2023-09-29T14:48:59.954877Z",
          "iopub.status.idle": "2023-09-29T14:48:59.970195Z",
          "shell.execute_reply": "2023-09-29T14:48:59.969352Z",
          "shell.execute_reply.started": "2023-09-29T14:48:59.955352Z"
        },
        "id": "uvghxBLyI4nR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "token = tokenizer(dataset['train'][0]['tokens'], is_split_into_words = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:59.972300Z",
          "iopub.status.busy": "2023-09-29T14:48:59.971656Z",
          "iopub.status.idle": "2023-09-29T14:48:59.985631Z",
          "shell.execute_reply": "2023-09-29T14:48:59.984714Z",
          "shell.execute_reply.started": "2023-09-29T14:48:59.972269Z"
        },
        "id": "k6sFjQaaI4nR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def align_target(labels, word_ids):\n",
        "    \"\"\"\n",
        "    Aligns target labels with tokenized word IDs, ensuring subword tokens\n",
        "    and special tokens receive appropriate labels.\n",
        "\n",
        "    Args:\n",
        "        labels (list): Original list of labels corresponding to words.\n",
        "        word_ids (list): List of word IDs from tokenization, where `None`\n",
        "                         represents special tokens.\n",
        "\n",
        "    Returns:\n",
        "        list: Aligned list of labels for tokenized word IDs.\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to store aligned labels\n",
        "    align_labels = []\n",
        "\n",
        "    # Variable to keep track of the last processed word ID\n",
        "    last_word = None\n",
        "\n",
        "    for word in word_ids:\n",
        "        if word is None:\n",
        "            # Assign -100 to special tokens (e.g., [CLS], [SEP])\n",
        "            label = -100\n",
        "        elif word != last_word:\n",
        "            # Use the label for the current word ID if it differs from the last one\n",
        "            label = labels[word]\n",
        "        else:\n",
        "            # Retain the label for subword tokens of the same word\n",
        "            label = labels[word]\n",
        "\n",
        "        # Append the determined label to the aligned labels list\n",
        "        align_labels.append(label)\n",
        "\n",
        "        # Update the last_word variable to the current word ID\n",
        "        last_word = word\n",
        "\n",
        "    return align_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:48:59.987899Z",
          "iopub.status.busy": "2023-09-29T14:48:59.987260Z",
          "iopub.status.idle": "2023-09-29T14:48:59.997327Z",
          "shell.execute_reply": "2023-09-29T14:48:59.996382Z",
          "shell.execute_reply.started": "2023-09-29T14:48:59.987869Z"
        },
        "id": "OOdNrO8WI4nS",
        "outputId": "d57842e5-61e2-46df-bcb4-4c9daac46220",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]'] \n",
            "--------------------------------------------------------------------------------------\n",
            " [3, 0, 7, 0, 0, 0, 7, 0, 0] \n",
            "--------------------------------------------------------------------------------------\n",
            " [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
          ]
        }
      ],
      "source": [
        "# Extracting labels and word_ids\n",
        "labels = dataset['train'][0]['ner_tags']\n",
        "word_ids = token.word_ids()\n",
        "\n",
        "# align_target function to align labels\n",
        "aligned_target = align_target(labels, word_ids)\n",
        "\n",
        "# Print tokenized tokens, original labels, and aligned labels\n",
        "print(token.tokens(), '\\n--------------------------------------------------------------------------------------\\n',\n",
        "      labels, '\\n--------------------------------------------------------------------------------------\\n',\n",
        "      aligned_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:00.006809Z",
          "iopub.status.busy": "2023-09-29T14:49:00.005332Z",
          "iopub.status.idle": "2023-09-29T14:49:00.027880Z",
          "shell.execute_reply": "2023-09-29T14:49:00.026341Z",
          "shell.execute_reply.started": "2023-09-29T14:49:00.006777Z"
        },
        "id": "0cAzj2MYI4nS",
        "outputId": "e6d6702f-b8f0-41ab-bd7c-6b6ffeae1601",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]\tNone\n",
            "EU\tB-ORG\n",
            "rejects\tO\n",
            "German\tB-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tO\n",
            "British\tB-MISC\n",
            "la\tO\n",
            "##mb\tO\n",
            ".\tO\n",
            "[SEP]\tNone\n"
          ]
        }
      ],
      "source": [
        "# Creating a list of aligned labels using label names\n",
        "aligned_labels = [label_names[t] if t >= 0 else None for t in aligned_target]\n",
        "\n",
        "# Loop through tokens and aligned labels and print them\n",
        "for x, y in zip(token.tokens(), aligned_labels):\n",
        "    print(f\"{x}\\t{y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:00.029813Z",
          "iopub.status.busy": "2023-09-29T14:49:00.028767Z",
          "iopub.status.idle": "2023-09-29T14:49:00.042949Z",
          "shell.execute_reply": "2023-09-29T14:49:00.042045Z",
          "shell.execute_reply.started": "2023-09-29T14:49:00.029571Z"
        },
        "id": "YWtHmplmI4nS",
        "outputId": "147f5501-2e50-4cb6-e00e-722e7155fd47",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]\tNone\n",
            "Ger\tB-MISC\n",
            "##man\tB-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "Micro\tB-ORG\n",
            "##so\tB-ORG\n",
            "##ft\tB-ORG\n",
            "[SEP]\tNone\n"
          ]
        }
      ],
      "source": [
        "# Define fake input data printing the labels\n",
        "\n",
        "words = ['[CLS]', 'Ger', '##man', 'call', 'to', 'Micro', '##so', '##ft', '[SEP]']\n",
        "word_ids = [None, 0, 0, 1, 2, 3, 3, 3, None]\n",
        "labels = [7, 0, 0, 3, 4]\n",
        "\n",
        "aligned_target = align_target(labels, word_ids)\n",
        "aligned_labels = [label_names[t] if t >= 0 else None for t in aligned_target]\n",
        "\n",
        "for x, y in zip(words, aligned_labels):\n",
        "    print(f\"{x}\\t{y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:00.046846Z",
          "iopub.status.busy": "2023-09-29T14:49:00.045727Z",
          "iopub.status.idle": "2023-09-29T14:49:00.067315Z",
          "shell.execute_reply": "2023-09-29T14:49:00.065910Z",
          "shell.execute_reply.started": "2023-09-29T14:49:00.046812Z"
        },
        "id": "A8QGBl5CI4nT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(batch):\n",
        "    \"\"\"\n",
        "    Tokenizes a batch of inputs and aligns the labels with the tokenized outputs.\n",
        "\n",
        "    Args:\n",
        "        batch (dict): A batch containing:\n",
        "            - 'tokens' (list of str): The input text tokens.\n",
        "            - 'ner_tags' (list of int): Corresponding labels for the tokens.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with tokenized inputs and aligned labels under the \"labels\" key.\n",
        "    \"\"\"\n",
        "    # Tokenize the input tokens with truncation and split into words\n",
        "    tokenized_inputs = tokenizer(\n",
        "        batch['tokens'], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    # Extract labels from the batch\n",
        "    labels_batch = batch['ner_tags']\n",
        "\n",
        "    # List to store aligned labels for each example\n",
        "    aligned_targets_batch = []\n",
        "\n",
        "    for i, labels in enumerate(labels_batch):\n",
        "        # Get word IDs for the current example\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "\n",
        "        # Align labels with tokenized word IDs\n",
        "        aligned_targets_batch.append(align_target(labels, word_ids))\n",
        "\n",
        "    # Add aligned labels to the tokenized inputs\n",
        "    tokenized_inputs[\"labels\"] = aligned_targets_batch\n",
        "\n",
        "    # Return the tokenized inputs with labels\n",
        "    return tokenized_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "91e7308090a54655a49cb7b1cabb3c7e"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:00.068993Z",
          "iopub.status.busy": "2023-09-29T14:49:00.068438Z",
          "iopub.status.idle": "2023-09-29T14:49:05.401791Z",
          "shell.execute_reply": "2023-09-29T14:49:05.399539Z",
          "shell.execute_reply.started": "2023-09-29T14:49:00.068963Z"
        },
        "id": "ppwvsHCaI4nT",
        "outputId": "4b9e822b-62b5-4839-bf49-97108d98b4cb",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c32cf5ccdcef416ca450f55c723fdb4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cab85db890db44df94d0d3ecd0ff104f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fdf42e06b844452b9ee428cc152af54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:05.408766Z",
          "iopub.status.busy": "2023-09-29T14:49:05.406457Z",
          "iopub.status.idle": "2023-09-29T14:49:14.625961Z",
          "shell.execute_reply": "2023-09-29T14:49:14.624948Z",
          "shell.execute_reply.started": "2023-09-29T14:49:05.408702Z"
        },
        "id": "8aRzod4cI4nT",
        "outputId": "28db256e-5511-49c0-e8ea-0ebf401f5332",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
              "           119,   102],\n",
              "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
              "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a DataCollatorForTokenClassification object\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Testing data using the data collator and display the resulting batch\n",
        "batch = data_collator([tokenized_dataset['train'][i] for i in range(2)])\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:26.332723Z",
          "iopub.status.busy": "2023-09-29T14:49:26.332376Z",
          "iopub.status.idle": "2023-09-29T14:49:27.072128Z",
          "shell.execute_reply": "2023-09-29T14:49:27.071158Z",
          "shell.execute_reply.started": "2023-09-29T14:49:26.332685Z"
        },
        "id": "idEIsUP4I4nU",
        "outputId": "f86ebfb8-35be-4328-da58-a95e32651f0d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\visha\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
              " 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
              " 'overall_precision': 0.0,\n",
              " 'overall_recall': 0.0,\n",
              " 'overall_f1': 0.0,\n",
              " 'overall_accuracy': 0.6666666666666666}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the seqeval metric which can evaluate NER and other sequence tasks\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "# List of List Input\n",
        "metric.compute(predictions = [['O' , 'B-ORG' , 'I-ORG']],\n",
        "               references = [['O' , 'B-MISC' , 'I-ORG']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:27.074465Z",
          "iopub.status.busy": "2023-09-29T14:49:27.073454Z",
          "iopub.status.idle": "2023-09-29T14:49:27.083093Z",
          "shell.execute_reply": "2023-09-29T14:49:27.081728Z",
          "shell.execute_reply.started": "2023-09-29T14:49:27.074432Z"
        },
        "id": "BwZ2Pji3I4nU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def compute_metrics(logits_and_labels):\n",
        "    \"\"\"\n",
        "    Computes evaluation metrics (precision, recall, F1-score, accuracy) for model predictions.\n",
        "\n",
        "    Args:\n",
        "        logits_and_labels (tuple): A tuple containing:\n",
        "            - logits (ndarray): Model output logits for each token.\n",
        "            - labels (list of lists): True labels for the tokens.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing precision, recall, F1-score, and accuracy.\n",
        "    \"\"\"\n",
        "    # Unpack logits and labels\n",
        "    logits, labels = logits_and_labels\n",
        "\n",
        "    # Convert logits to predicted labels by taking the argmax\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (-100) from labels and map to label names\n",
        "    str_labels = [\n",
        "        [label_names[t] for t in label if t != -100]\n",
        "        for label in labels\n",
        "    ]\n",
        "\n",
        "    # Map predictions to label names while ignoring special token indices\n",
        "    str_preds = [\n",
        "        [label_names[p] for (p, t) in zip(prediction, label) if t != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # Compute metrics using the evaluation metric object\n",
        "    results = metric.compute(predictions=str_preds, references=str_labels)\n",
        "\n",
        "    # Extract and return key metrics\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:27.084786Z",
          "iopub.status.busy": "2023-09-29T14:49:27.084455Z",
          "iopub.status.idle": "2023-09-29T14:49:27.094943Z",
          "shell.execute_reply": "2023-09-29T14:49:27.094033Z",
          "shell.execute_reply.started": "2023-09-29T14:49:27.084755Z"
        },
        "id": "ZHb6gm7NI4nV",
        "outputId": "001b6ed1-9096-40d9-b7c8-212637c6ea2f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'} \n",
            "--------------------\n",
            " {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
          ]
        }
      ],
      "source": [
        "# Create mapping from label ID to label string name\n",
        "id2label = {k: v for k, v in enumerate(label_names)}\n",
        "\n",
        "# Create reverse mapping from label name to label ID\n",
        "label2id = {v: k for k, v in enumerate(label_names)}\n",
        "\n",
        "print(id2label , '\\n--------------------\\n' , label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:27.097552Z",
          "iopub.status.busy": "2023-09-29T14:49:27.096642Z",
          "iopub.status.idle": "2023-09-29T14:49:29.799171Z",
          "shell.execute_reply": "2023-09-29T14:49:29.798275Z",
          "shell.execute_reply.started": "2023-09-29T14:49:27.097518Z"
        },
        "id": "vi8MDaG8I4nV",
        "outputId": "e497f014-8a0d-49b4-c4c8-5a417574e0ec",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\visha\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Initialize model object with pretrained weights\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "  model_name,\n",
        "\n",
        "  # Pass in label mappings\n",
        "  id2label=id2label,\n",
        "  label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxdUqWBEZ_9n"
      },
      "source": [
        "### Training model\n",
        "The fine-tuning process utilized the pre-trained bert-base-cased BERT model with the following setup:\n",
        "\n",
        "**Batch Size:** Set to 16 to strike a balance between memory usage and training efficiency.\n",
        "\n",
        "**Optimizer:** Used the AdamW optimizer with:\n",
        "- **Learning Rate:** 2e-5.\n",
        "- **Weight Decay:** 0.01 for regularization.\n",
        "\n",
        "**Epochs:** Trained for 10 epochs, with evaluation conducted at the end of each epoch to monitor performance on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:29.800993Z",
          "iopub.status.busy": "2023-09-29T14:49:29.800666Z",
          "iopub.status.idle": "2023-09-29T14:49:29.839152Z",
          "shell.execute_reply": "2023-09-29T14:49:29.838147Z",
          "shell.execute_reply.started": "2023-09-29T14:49:29.800962Z"
        },
        "id": "d_tci2WhI4nV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Configure training arguments using TrainigArguments class\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = \"fine_tuned_model\",\n",
        "\n",
        "  # Evaluate each epoch\n",
        "  evaluation_strategy = \"epoch\",\n",
        "\n",
        "  # Learning rate for Adam optimizer\n",
        "  learning_rate = 2e-5,\n",
        "\n",
        "  # Batch sizes for training and evaluation\n",
        "  per_device_train_batch_size = 16,\n",
        "  per_device_eval_batch_size = 16,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs = 10,\n",
        "\n",
        "  # L2 weight decay regularization\n",
        "  weight_decay = 0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:29.841865Z",
          "iopub.status.busy": "2023-09-29T14:49:29.841318Z",
          "iopub.status.idle": "2023-09-29T14:49:35.215400Z",
          "shell.execute_reply": "2023-09-29T14:49:35.214479Z",
          "shell.execute_reply.started": "2023-09-29T14:49:29.841833Z"
        },
        "id": "-XbWfwT5I4nV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize Trainer object for model training\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "\n",
        "  # Training arguments\n",
        "  args=training_args,\n",
        "\n",
        "  # Training and validation datasets\n",
        "  train_dataset=tokenized_dataset[\"train\"],\n",
        "  eval_dataset=tokenized_dataset[\"test\"],\n",
        "\n",
        "  # Tokenizer\n",
        "  tokenizer=tokenizer,\n",
        "\n",
        "  # Custom metric function\n",
        "  compute_metrics=compute_metrics,\n",
        "\n",
        "  # Data collator\n",
        "  data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d66e1169f1d04ec99f6ca81c2e2a5efa",
            "f51039ff01b74ee18785d9ab25cf054d",
            "0f9d38784bc44df3a54834cc7032b382",
            "17f84e45a00748a08cd635f37122801f",
            "44ad2cef99224bc3b68f4d7422451b31",
            "bf3e0d151f074a799acea8cc8abe5e0d",
            "71648eb3ef0548c6a761aadbd7efbe38",
            "200a6b9ed3eb401ba8e6462d1173b58a",
            "f24cb69b0d6a46b18dec47603315990e",
            "a0b9fbfa7ec04bd2854e6fde2da09c12",
            "aeafe42bd21149c8b240a84bda715f5a"
          ]
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2023-09-29T14:49:35.216818Z",
          "iopub.status.busy": "2023-09-29T14:49:35.216515Z",
          "iopub.status.idle": "2023-09-29T14:55:15.112378Z",
          "shell.execute_reply": "2023-09-29T14:55:15.110830Z",
          "shell.execute_reply.started": "2023-09-29T14:49:35.216789Z"
        },
        "id": "OZDiHPZhI4nW",
        "outputId": "da86b2ae-85a5-422f-964d-3427939d99b7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\visha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 14041\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 8780\n",
            "  Number of trainable parameters = 107726601\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "192bdb3fa4ae43e2a1ac3e12a9ba5d54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8780 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.235, 'learning_rate': 1.886104783599089e-05, 'epoch': 0.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-500\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e748da0d721b4f68bad61a778c2910de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.14176404476165771, 'eval_precision': 0.885834109972041, 'eval_recall': 0.8818072177381946, 'eval_f1': 0.883816076991027, 'eval_accuracy': 0.9682324577299444, 'eval_runtime': 10.7236, 'eval_samples_per_second': 322.001, 'eval_steps_per_second': 20.143, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-1000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0811, 'learning_rate': 1.7722095671981778e-05, 'epoch': 1.14}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-1000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-1000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-1000\\special_tokens_map.json\n",
            "Saving model checkpoint to fine_tuned_model\\checkpoint-1500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-1500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0531, 'learning_rate': 1.6583143507972667e-05, 'epoch': 1.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-1500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-1500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-1500\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "142a87ea147a46c0b0a65c3246d9b36d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.17270991206169128, 'eval_precision': 0.8797263233705438, 'eval_recall': 0.90657760460154, 'eval_f1': 0.8929501530588934, 'eval_accuracy': 0.9690991317502088, 'eval_runtime': 9.1111, 'eval_samples_per_second': 378.988, 'eval_steps_per_second': 23.707, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-2000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-2000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.035, 'learning_rate': 1.5444191343963555e-05, 'epoch': 2.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-2000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-2000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-2000\\special_tokens_map.json\n",
            "Saving model checkpoint to fine_tuned_model\\checkpoint-2500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-2500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0261, 'learning_rate': 1.4305239179954442e-05, 'epoch': 2.85}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-2500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-2500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-2500\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d1bdb6cd66d4a4f943ff6949777f259",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.19291624426841736, 'eval_precision': 0.8862885462555066, 'eval_recall': 0.8959087113832452, 'eval_f1': 0.8910726643598617, 'eval_accuracy': 0.9684215502434567, 'eval_runtime': 8.8889, 'eval_samples_per_second': 388.463, 'eval_steps_per_second': 24.3, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-3000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-3000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0209, 'learning_rate': 1.3166287015945332e-05, 'epoch': 3.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-3000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-3000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-3000\\special_tokens_map.json\n",
            "Saving model checkpoint to fine_tuned_model\\checkpoint-3500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-3500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0176, 'learning_rate': 1.2027334851936218e-05, 'epoch': 3.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-3500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-3500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-3500\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9fd49a914ca4b60bfe58a426165fdc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.19878941774368286, 'eval_precision': 0.8831521739130435, 'eval_recall': 0.904536598942388, 'eval_f1': 0.8937164856317888, 'eval_accuracy': 0.9689573123650747, 'eval_runtime': 8.992, 'eval_samples_per_second': 384.008, 'eval_steps_per_second': 24.021, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-4000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-4000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0098, 'learning_rate': 1.0888382687927108e-05, 'epoch': 4.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-4000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-4000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-4000\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af3d81679b944a74b4567161045af38c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.20481108129024506, 'eval_precision': 0.888242247885787, 'eval_recall': 0.9062065126635124, 'eval_f1': 0.8971344599559148, 'eval_accuracy': 0.9706433872772254, 'eval_runtime': 8.9958, 'eval_samples_per_second': 383.845, 'eval_steps_per_second': 24.011, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-4500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-4500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0102, 'learning_rate': 9.749430523917997e-06, 'epoch': 5.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-4500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-4500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-4500\\special_tokens_map.json\n",
            "Saving model checkpoint to fine_tuned_model\\checkpoint-5000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-5000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0062, 'learning_rate': 8.610478359908885e-06, 'epoch': 5.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-5000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-5000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-5000\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cc17ebefb324a9ea90ad04c2bf87bc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.22591976821422577, 'eval_precision': 0.8874411871154542, 'eval_recall': 0.9099174320437888, 'eval_f1': 0.8985387751362741, 'eval_accuracy': 0.9709427837569531, 'eval_runtime': 9.0417, 'eval_samples_per_second': 381.896, 'eval_steps_per_second': 23.889, 'epoch': 6.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-5500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-5500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0053, 'learning_rate': 7.471526195899773e-06, 'epoch': 6.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-5500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-5500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-5500\\special_tokens_map.json\n",
            "Saving model checkpoint to fine_tuned_model\\checkpoint-6000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-6000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0041, 'learning_rate': 6.3325740318906616e-06, 'epoch': 6.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-6000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-6000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-6000\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fc34c54038442ea96a509d3914c85ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.23265650868415833, 'eval_precision': 0.8903261067589725, 'eval_recall': 0.9067631505705539, 'eval_f1': 0.8984694581054374, 'eval_accuracy': 0.9703282330880383, 'eval_runtime': 9.1629, 'eval_samples_per_second': 376.847, 'eval_steps_per_second': 23.573, 'epoch': 7.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-6500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-6500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0028, 'learning_rate': 5.19362186788155e-06, 'epoch': 7.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-6500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-6500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-6500\\special_tokens_map.json\n",
            "Saving model checkpoint to fine_tuned_model\\checkpoint-7000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-7000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0022, 'learning_rate': 4.054669703872437e-06, 'epoch': 7.97}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-7000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-7000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-7000\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6faaec483b144e2a5a06c88404fe552",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.24634404480457306, 'eval_precision': 0.8907868435398874, 'eval_recall': 0.9095463401057612, 'eval_f1': 0.9000688547165481, 'eval_accuracy': 0.9707694489529002, 'eval_runtime': 9.1047, 'eval_samples_per_second': 379.256, 'eval_steps_per_second': 23.724, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-7500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-7500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0013, 'learning_rate': 2.9157175398633257e-06, 'epoch': 8.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-7500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-7500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-7500\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cffefa479c574e849231d424add66942",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2611589729785919, 'eval_precision': 0.8873379860418744, 'eval_recall': 0.9082475183226645, 'eval_f1': 0.8976710067852558, 'eval_accuracy': 0.9699815634799326, 'eval_runtime': 9.4246, 'eval_samples_per_second': 366.383, 'eval_steps_per_second': 22.919, 'epoch': 9.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\\checkpoint-8000\n",
            "Configuration saved in fine_tuned_model\\checkpoint-8000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0013, 'learning_rate': 1.7767653758542143e-06, 'epoch': 9.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-8000\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-8000\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-8000\\special_tokens_map.json\n",
            "Saving model checkpoint to fine_tuned_model\\checkpoint-8500\n",
            "Configuration saved in fine_tuned_model\\checkpoint-8500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0012, 'learning_rate': 6.378132118451026e-07, 'epoch': 9.68}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in fine_tuned_model\\checkpoint-8500\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\checkpoint-8500\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\checkpoint-8500\\special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "740129e6e9344b2f9c80aab55019940e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.26006409525871277, 'eval_precision': 0.887920863962247, 'eval_recall': 0.907690880415623, 'eval_f1': 0.89769703642536, 'eval_accuracy': 0.9700130788988512, 'eval_runtime': 9.5399, 'eval_samples_per_second': 361.952, 'eval_steps_per_second': 22.642, 'epoch': 10.0}\n",
            "{'train_runtime': 1399.6152, 'train_samples_per_second': 100.32, 'train_steps_per_second': 6.273, 'train_loss': 0.029273570748152115, 'epoch': 10.0}\n",
            "Training time: 1400.7874233722687 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Training time: {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2023-09-29T14:55:15.117568Z",
          "iopub.status.busy": "2023-09-29T14:55:15.116883Z",
          "iopub.status.idle": "2023-09-29T14:55:15.694566Z",
          "shell.execute_reply": "2023-09-29T14:55:15.693454Z",
          "shell.execute_reply.started": "2023-09-29T14:55:15.117530Z"
        },
        "id": "_1qVAUx6I4nW",
        "outputId": "d3b9540d-01d7-4316-923a-1323d5b0b879",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to fine_tuned_model\n",
            "Configuration saved in fine_tuned_model\\config.json\n",
            "Model weights saved in fine_tuned_model\\pytorch_model.bin\n",
            "tokenizer config file saved in fine_tuned_model\\tokenizer_config.json\n",
            "Special tokens file saved in fine_tuned_model\\special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model('fine_tuned_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF5DF4E0Z0AM"
      },
      "source": [
        "### Creating model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "Hhv_m9tQI4nW",
        "outputId": "be6a0881-74fc-431d-e87b-ce105e3421a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file fine_tuned_model\\config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fine_tuned_model\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-PER\",\n",
            "    \"2\": \"I-PER\",\n",
            "    \"3\": \"B-ORG\",\n",
            "    \"4\": \"I-ORG\",\n",
            "    \"5\": \"B-LOC\",\n",
            "    \"6\": \"I-LOC\",\n",
            "    \"7\": \"B-MISC\",\n",
            "    \"8\": \"I-MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 5,\n",
            "    \"B-MISC\": 7,\n",
            "    \"B-ORG\": 3,\n",
            "    \"B-PER\": 1,\n",
            "    \"I-LOC\": 6,\n",
            "    \"I-MISC\": 8,\n",
            "    \"I-ORG\": 4,\n",
            "    \"I-PER\": 2,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file fine_tuned_model\\pytorch_model.bin\n",
            "c:\\Users\\visha\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
            "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "All the weights of BertForTokenClassification were initialized from the model checkpoint at fine_tuned_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert.embeddings.word_embeddings.weight: torch.Size([28996, 768])\n",
            "bert.embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
            "bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
            "bert.embeddings.LayerNorm.weight: torch.Size([768])\n",
            "bert.embeddings.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
            "bert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
            "bert.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
            "bert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
            "classifier.weight: torch.Size([9, 768])\n",
            "classifier.bias: torch.Size([9])\n",
            "Total number of parameters: 107726601\n"
          ]
        }
      ],
      "source": [
        "# Load the configuration and model\n",
        "config = AutoConfig.from_pretrained('fine_tuned_model', local_files_only=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained('fine_tuned_model', config=config, local_files_only=True)\n",
        "\n",
        "# Print model parameters and their shapes\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape}\")\n",
        "\n",
        "# Print total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2023-09-29T14:55:15.696379Z",
          "iopub.status.busy": "2023-09-29T14:55:15.695850Z",
          "iopub.status.idle": "2023-09-29T14:55:18.335813Z",
          "shell.execute_reply": "2023-09-29T14:55:18.334522Z",
          "shell.execute_reply.started": "2023-09-29T14:55:15.696341Z"
        },
        "id": "vBUoLYuFI4nW",
        "outputId": "35d211c1-2377-4d8c-bd50-1ef523713ea9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file fine_tuned_model\\config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fine_tuned_model\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-PER\",\n",
            "    \"2\": \"I-PER\",\n",
            "    \"3\": \"B-ORG\",\n",
            "    \"4\": \"I-ORG\",\n",
            "    \"5\": \"B-LOC\",\n",
            "    \"6\": \"I-LOC\",\n",
            "    \"7\": \"B-MISC\",\n",
            "    \"8\": \"I-MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 5,\n",
            "    \"B-MISC\": 7,\n",
            "    \"B-ORG\": 3,\n",
            "    \"B-PER\": 1,\n",
            "    \"I-LOC\": 6,\n",
            "    \"I-MISC\": 8,\n",
            "    \"I-ORG\": 4,\n",
            "    \"I-PER\": 2,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file fine_tuned_model\\config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fine_tuned_model\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-PER\",\n",
            "    \"2\": \"I-PER\",\n",
            "    \"3\": \"B-ORG\",\n",
            "    \"4\": \"I-ORG\",\n",
            "    \"5\": \"B-LOC\",\n",
            "    \"6\": \"I-LOC\",\n",
            "    \"7\": \"B-MISC\",\n",
            "    \"8\": \"I-MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 5,\n",
            "    \"B-MISC\": 7,\n",
            "    \"B-ORG\": 3,\n",
            "    \"B-PER\": 1,\n",
            "    \"I-LOC\": 6,\n",
            "    \"I-MISC\": 8,\n",
            "    \"I-ORG\": 4,\n",
            "    \"I-PER\": 2,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file fine_tuned_model\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "All the weights of BertForTokenClassification were initialized from the model checkpoint at fine_tuned_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "ner = pipeline(\n",
        "    'token-classification',\n",
        "    model = 'fine_tuned_model',\n",
        "    aggregation_strategy = 'simple' ,\n",
        "    device = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2023-09-29T15:05:22.749813Z",
          "iopub.status.busy": "2023-09-29T15:05:22.749477Z",
          "iopub.status.idle": "2023-09-29T15:05:22.773415Z",
          "shell.execute_reply": "2023-09-29T15:05:22.772205Z",
          "shell.execute_reply.started": "2023-09-29T15:05:22.749787Z"
        },
        "id": "lK7yAc4GI4nX",
        "outputId": "60420f2b-439e-488f-c40d-260bb28e328b",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'ORG',\n",
              "  'score': 0.99988127,\n",
              "  'word': 'EU',\n",
              "  'start': 0,\n",
              "  'end': 2},\n",
              " {'entity_group': 'MISC',\n",
              "  'score': 0.9998983,\n",
              "  'word': 'German',\n",
              "  'start': 11,\n",
              "  'end': 17},\n",
              " {'entity_group': 'MISC',\n",
              "  'score': 0.99990785,\n",
              "  'word': 'British',\n",
              "  'start': 34,\n",
              "  'end': 41}]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner('EU rejects German call to boycott British lamb .')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "Hiki_D5JI4nX",
        "outputId": "601f8238-edfb-4cc2-b7ec-06dd0e43cb0e"
      },
      "outputs": [],
      "source": [
        "# Original sentences (from CoNLL dataset or similar for demonstration)\n",
        "original_sentences = [\n",
        "    \"Barack Obama was born in Honolulu, Hawaii.\",\n",
        "    \"Apple Inc. is based in Cupertino, California.\",\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"Cristiano Ronaldo scored a goal for Portugal.\",\n",
        "    \"Tesla manufactures electric cars in California.\",\n",
        "    \"Google was founded by Larry Page and Sergey Brin.\",\n",
        "    \"Microsoft announced a new product yesterday.\",\n",
        "    \"Amazon's headquarters are in Seattle, Washington.\",\n",
        "    \"The Prime Minister met with the President of France.\",\n",
        "    \"The Grand Canyon is a major tourist attraction in Arizona.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "331n9LzlVorz"
      },
      "outputs": [],
      "source": [
        "# Define a function for synonym replacement as an adversarial transformation\n",
        "def synonym_replacement(sentence):\n",
        "    \"\"\"\n",
        "    Replaces a random word in the sentence with one of its synonyms.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to be transformed.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentence with one word replaced by its synonym.\n",
        "    \"\"\"\n",
        "    # Split the sentence into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Randomly select a word to replace\n",
        "    word_to_replace = random.choice(words)\n",
        "\n",
        "    # Get synonyms for the selected word from WordNet\n",
        "    synonyms = wordnet.synsets(word_to_replace)\n",
        "\n",
        "    # If synonyms exist, replace the word with the first synonym found\n",
        "    if synonyms:\n",
        "        synonym = synonyms[0].lemmas()[0].name()\n",
        "        # Ensure the synonym is different from the original word\n",
        "        if synonym != word_to_replace:\n",
        "            words[words.index(word_to_replace)] = synonym\n",
        "\n",
        "    # Return the modified sentence\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2vzju2wEVooY"
      },
      "outputs": [],
      "source": [
        "# Define a function to introduce a random typo in a sentence\n",
        "def introduce_typo(sentence):\n",
        "    \"\"\"\n",
        "    Introduces a random typo in a randomly selected word in the sentence.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to be transformed.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentence with one word modified to introduce a typo.\n",
        "    \"\"\"\n",
        "    # Split the sentence into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Randomly select a word to introduce a typo\n",
        "    word_to_typo = random.choice(words)\n",
        "\n",
        "    # If the word is longer than one character, introduce a typo\n",
        "    if len(word_to_typo) > 1:\n",
        "        typo = list(word_to_typo)\n",
        "        # Replace a random character in the word with a random letter\n",
        "        typo[random.randint(0, len(typo)-1)] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "        # Update the word in the sentence\n",
        "        words[words.index(word_to_typo)] = \"\".join(typo)\n",
        "\n",
        "    # Return the modified sentence\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9GrjuFYoVoli"
      },
      "outputs": [],
      "source": [
        "# Define a function to swap named entities in a sentence\n",
        "def entity_swapping(sentence):\n",
        "    \"\"\"\n",
        "    Swaps predefined entities in the sentence with other entities.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence containing named entities.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentence with entities swapped if found.\n",
        "    \"\"\"\n",
        "    # List of original entities and their corresponding replacements\n",
        "    entities = [\"Barack Obama\", \"Honolulu\", \"Apple Inc.\", \"Cupertino\", \"Paris\",\n",
        "                \"Cristiano Ronaldo\", \"Tesla\", \"Google\", \"Microsoft\", \"Seattle\"]\n",
        "    replacements = [\"Joe Biden\", \"Kailua\", \"Google\", \"Mountain View\", \"London\",\n",
        "                    \"Lionel Messi\", \"Lucid\", \"Facebook\", \"Amazon\", \"Portland\"]\n",
        "\n",
        "    # Loop through entities and their replacements\n",
        "    for original, replacement in zip(entities, replacements):\n",
        "        # If the original entity is found, replace it with the corresponding replacement\n",
        "        if original in sentence:\n",
        "            return sentence.replace(original, replacement)\n",
        "\n",
        "    # Return the sentence unchanged if no entity is found\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-tWS_hLgVoib"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: Barack Obama was born in Honolulu, Hawaii.\n",
            "Adversarial: Joe Biden was born in Honolulu, Hawaii.\n",
            "\n",
            "Original: Apple Inc. is based in Cupertino, California.\n",
            "Adversarial: apple Inc. is based in Cupertino, California.\n",
            "\n",
            "Original: The Eiffel Tower is located in Paris.\n",
            "Adversarial: The Eiffel Tower is located in London.\n",
            "\n",
            "Original: Cristiano Ronaldo scored a goal for Portugal.\n",
            "Adversarial: Cristiano Rhnaldo scored a goal for Portugal.\n",
            "\n",
            "Original: Tesla manufactures electric cars in California.\n",
            "Adversarial: Lucid manufactures electric cars in California.\n",
            "\n",
            "Original: Google was founded by Larry Page and Sergey Brin.\n",
            "Adversarial: Google was founded by Larry Page and Sergey Brin.\n",
            "\n",
            "Original: Microsoft announced a new product yesterday.\n",
            "Adversarial: Microsoft announced a new merchandise yesterday.\n",
            "\n",
            "Original: Amazon's headquarters are in Seattle, Washington.\n",
            "Adversarial: Aaazon's headquarters are in Seattle, Washington.\n",
            "\n",
            "Original: The Prime Minister met with the President of France.\n",
            "Adversarial: The Prime Minister meet with the President of France.\n",
            "\n",
            "Original: The Grand Canyon is a major tourist attraction in Arizona.\n",
            "Adversarial: The Grand Canyon is a major tourist attraction in Arizona.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate adversarial examples\n",
        "adversarial_pairs = []\n",
        "for sentence in original_sentences:\n",
        "    transformation = random.choice([synonym_replacement, introduce_typo, entity_swapping])\n",
        "    adversarial_sentence = transformation(sentence)\n",
        "    adversarial_pairs.append((sentence, adversarial_sentence))\n",
        "\n",
        "# Display results\n",
        "for original, adversarial in adversarial_pairs:\n",
        "    print(f\"Original: {original}\")\n",
        "    print(f\"Adversarial: {adversarial}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da90J_TbZJro"
      },
      "source": [
        "Compares Named Entity Recognition (NER) results between original and adversarial texts.\n",
        "\n",
        "For each pair of sentences:\n",
        "- Prints the original sentence.\n",
        "- Prints the adversarially transformed sentence.\n",
        "- Performs NER on the adversarial sentence and displays the detected entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "wLmETg5tI4nX",
        "outputId": "0335ae85-0221-47c3-fb8d-d7b2033b663c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Obama was born in Honolulu, Hawaii.\n",
            "Joe Biden was born in Honolulu, Hawaii.\n",
            "NER Results:\n",
            "Joe Biden PER\n",
            "Honolulu LOC\n",
            "Hawaii LOC\n",
            "\n",
            "\n",
            "Apple Inc. is based in Cupertino, California.\n",
            "apple Inc. is based in Cupertino, California.\n",
            "NER Results:\n",
            "apple Inc. ORG\n",
            "Cup LOC\n",
            "##ert LOC\n",
            "##ino LOC\n",
            "California LOC\n",
            "\n",
            "\n",
            "The Eiffel Tower is located in Paris.\n",
            "The Eiffel Tower is located in London.\n",
            "NER Results:\n",
            "E LOC\n",
            "##iff LOC\n",
            "##el Tower LOC\n",
            "London LOC\n",
            "\n",
            "\n",
            "Cristiano Ronaldo scored a goal for Portugal.\n",
            "Cristiano Rhnaldo scored a goal for Portugal.\n",
            "NER Results:\n",
            "C PER\n",
            "##rist PER\n",
            "##iano Rhnaldo PER\n",
            "Portugal LOC\n",
            "\n",
            "\n",
            "Tesla manufactures electric cars in California.\n",
            "Lucid manufactures electric cars in California.\n",
            "NER Results:\n",
            "Luc ORG\n",
            "##id ORG\n",
            "California LOC\n",
            "\n",
            "\n",
            "Google was founded by Larry Page and Sergey Brin.\n",
            "Google was founded by Larry Page and Sergey Brin.\n",
            "NER Results:\n",
            "Google ORG\n",
            "Larry Page PER\n",
            "Sergey Brin PER\n",
            "\n",
            "\n",
            "Microsoft announced a new product yesterday.\n",
            "Microsoft announced a new merchandise yesterday.\n",
            "NER Results:\n",
            "Microsoft ORG\n",
            "\n",
            "\n",
            "Amazon's headquarters are in Seattle, Washington.\n",
            "Aaazon's headquarters are in Seattle, Washington.\n",
            "NER Results:\n",
            "A ORG\n",
            "##aa ORG\n",
            "##zon ORG\n",
            "Seattle LOC\n",
            "Washington LOC\n",
            "\n",
            "\n",
            "The Prime Minister met with the President of France.\n",
            "The Prime Minister meet with the President of France.\n",
            "NER Results:\n",
            "France LOC\n",
            "\n",
            "\n",
            "The Grand Canyon is a major tourist attraction in Arizona.\n",
            "The Grand Canyon is a major tourist attraction in Arizona.\n",
            "NER Results:\n",
            "Grand Canyon LOC\n",
            "Arizona LOC\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\visha\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Iterate over original and adversarial sentence pairs\n",
        "for original, adversarial in adversarial_pairs:\n",
        "    # Print the original sentence\n",
        "    print(original)\n",
        "\n",
        "    # Print the adversarial sentence\n",
        "    print(adversarial)\n",
        "\n",
        "    # Perform NER and display results\n",
        "    print(\"NER Results:\")\n",
        "    result = ner(adversarial)\n",
        "    for entity in result:\n",
        "        # Print each detected entity and its predicted label group\n",
        "        print(entity['word'], entity['entity_group'])\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
